{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160f74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch script for model training (MobileNetV2).\n",
    "Copyright (C) 2021 by Akira TAMAMORI\n",
    "Permission is hereby granted, free of charge, to any person obtaining\n",
    "a copy of this software and associated documentation files (the\n",
    "\"Software\"), to deal in the Software without restriction, including\n",
    "without limitation the rights to use, copy, modify, merge, publish,\n",
    "distribute, sublicense, and/or sell copies of the Software, and to\n",
    "permit persons to whom the Software is furnished to do so, subject to\n",
    "the following conditions:\n",
    "The above copyright notice and this permission notice shall be\n",
    "included in all copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    "OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    "WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports.\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from itertools import chain\n",
    "import random\n",
    "# Related third party imports.\n",
    "import joblib\n",
    "import numpy\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from scipy.special import softmax\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local application/library specific imports.\n",
    "import util\n",
    "from pytorch_model import Transformer_Decoder as Model\n",
    "\n",
    "# Load configuration from YAML file.\n",
    "CONFIG = util.load_yaml(\"./config.yaml\")\n",
    "\n",
    "# String constant: \"cuda:0\" or \"cpu\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a163e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    numpy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f36b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    \"\"\"\n",
    "    Flatten list.\n",
    "    \"\"\"\n",
    "    return list(chain.from_iterable(nested_list))\n",
    "\n",
    "\n",
    "def concat_features(file_list):\n",
    "    \"\"\"\n",
    "    Extract features from audio files and then concate them into an array.\n",
    "    \"\"\"\n",
    "    # calculate the number of dimensions\n",
    "    n_features = []\n",
    "    for file_id, file_name in enumerate(file_list):\n",
    "\n",
    "        # extract feature from audio file.\n",
    "        feature = util.extract_feature(file_name, CONFIG[\"feature\"])\n",
    "        feature = feature[:: CONFIG[\"feature\"][\"n_hop_frames\"], :]\n",
    "\n",
    "        if file_id == 0:\n",
    "            features = numpy.zeros(\n",
    "                (\n",
    "                    len(file_list) * feature.shape[0],\n",
    "                    CONFIG[\"feature\"][\"n_mels\"] * CONFIG[\"feature\"][\"n_frames\"],\n",
    "                ),\n",
    "                dtype=float,\n",
    "            )\n",
    "\n",
    "        features[\n",
    "            feature.shape[0] * file_id : feature.shape[0] * (file_id + 1), :\n",
    "        ] = feature\n",
    "\n",
    "        n_features.append(feature.shape[0])\n",
    "\n",
    "    return features, n_features\n",
    "\n",
    "\n",
    "class DcaseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Prepare dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_section_names, target_dir, mode):\n",
    "        super().__init__()\n",
    "\n",
    "        n_files_ea_section = []  # number of files for each section\n",
    "        n_vectors_ea_file = []  # number of vectors for each file\n",
    "        data = numpy.empty(\n",
    "            (0, CONFIG[\"feature\"][\"n_frames\"] * CONFIG[\"feature\"][\"n_mels\"]),\n",
    "            dtype=float,\n",
    "        )\n",
    "        \n",
    "        for section_name in unique_section_names:\n",
    "            # get file list for each section\n",
    "            # all values of y_true are zero in training\n",
    "            print(\"target_dir : %s\" % (target_dir + \"_\" + section_name))\n",
    "            files, _ = util.file_list_generator(\n",
    "                target_dir=target_dir,\n",
    "                section_name=section_name,\n",
    "                dir_name=\"train\",\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\"number of files : %s\" % (str(len(files))))\n",
    "\n",
    "            n_files_ea_section.append(len(files))\n",
    "\n",
    "            # extract features from audio files and\n",
    "            # concatenate them into Numpy array.\n",
    "            features, n_features = concat_features(files)\n",
    "\n",
    "            data = numpy.append(data, features, axis=0)\n",
    "            n_vectors_ea_file.append(n_features)\n",
    "\n",
    "        n_vectors_ea_file = flatten(n_vectors_ea_file)\n",
    "\n",
    "        # make target labels for conditioning\n",
    "        # they are not one-hot vector!\n",
    "        labels = numpy.zeros((data.shape[0]), dtype=int)\n",
    "        start_index = 0\n",
    "        for section_index in range(unique_section_names.shape[0]):\n",
    "            for file_id in range(n_files_ea_section[section_index]):\n",
    "                labels[\n",
    "                    start_index : start_index + n_vectors_ea_file[file_id]\n",
    "                ] = section_index\n",
    "                start_index += n_vectors_ea_file[file_id]\n",
    "\n",
    "        # 1D vector to 2D image (1ch)\n",
    "        self.data = data.reshape(\n",
    "            (\n",
    "                data.shape[0],\n",
    "                #1,  # number of channels\n",
    "                CONFIG[\"feature\"][\"n_frames\"],\n",
    "                CONFIG[\"feature\"][\"n_mels\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]  # return num of samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index, :]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a325e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset):\n",
    "    \"\"\"\n",
    "    Make dataloader from dataset for training.\n",
    "    \"\"\"\n",
    "    train_size = int(len(dataset) * (1.0 - CONFIG[\"training\"][\"validation_split\"]))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size])\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
    "        shuffle=CONFIG[\"training\"][\"shuffle\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # dataloader of training data for evaluation only\n",
    "    data_loader_eval_train = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    return data_loader_train, data_loader_val, data_loader_eval_train\n",
    "\n",
    "\n",
    "def get_model(embed_dim, seq_len):\n",
    "    \"\"\"\n",
    "    Instantiate AutoEncoder.\n",
    "    \"\"\"\n",
    "    model = Model(embed_dim, seq_len-1).to(DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Instantiate optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        params=model.parameters(),\n",
    "        weight_decay=CONFIG[\"training\"][\"weight_decay\"],\n",
    "        lr=CONFIG[\"training\"][\"learning_rate\"],\n",
    "    )\n",
    "\n",
    "    # optional\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=CONFIG[\"training\"][\"lr_step_size\"],\n",
    "        gamma=CONFIG[\"training\"][\"lr_gamma\"],\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507ab978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_anomaly_score(model, file_path, section_index):\n",
    "    \"\"\"\n",
    "    Calculate anomaly score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # extract features (log-mel spectrogram)\n",
    "        data = util.extract_feature(file_name=file_path, config=CONFIG[\"feature\"])\n",
    "        data = data.reshape(\n",
    "            (  # must be a tuple of ints\n",
    "                data.shape[0],\n",
    "                1,\n",
    "                CONFIG[\"feature\"][\"n_frames\"],\n",
    "                CONFIG[\"feature\"][\"n_mels\"],\n",
    "            )\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"File broken!!: {}\".format(file_path))\n",
    "\n",
    "    condition = numpy.zeros((data.shape[0]), dtype=int)\n",
    "    if section_index != -1:\n",
    "        condition[:] = section_index\n",
    "\n",
    "    feed_data = torch.from_numpy(data).clone()\n",
    "    feed_data = feed_data.to(DEVICE).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(feed_data)  # notice: unnormalized output\n",
    "        output = output.to(\"cpu\").detach().numpy().copy()  # tensor to numpy array.\n",
    "\n",
    "    output = softmax(output, axis=1)\n",
    "    prob = output[:, section_index]\n",
    "\n",
    "    y_pred = numpy.mean(\n",
    "        numpy.log(\n",
    "            numpy.maximum(1.0 - prob, sys.float_info.epsilon)\n",
    "            - numpy.log(numpy.maximum(prob, sys.float_info.epsilon))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def fit_gamma_dist(model, target_dir, mode):\n",
    "    \"\"\"\n",
    "    - Calculate anomaly scores over sections.\n",
    "    - Fit gamma distribution for anomaly scores.\n",
    "    - Save the parameters of the distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    section_names = util.get_section_names(target_dir, dir_name=\"train\")\n",
    "    dataset_scores = numpy.array([], dtype=numpy.float64)\n",
    "\n",
    "    # calculate anomaly scores over sections\n",
    "    for section_index, section_name in enumerate(section_names):\n",
    "        section_files, _ = util.file_list_generator(\n",
    "            target_dir=target_dir,\n",
    "            section_name=section_name,\n",
    "            dir_name=\"train\",\n",
    "            mode=mode,\n",
    "        )\n",
    "        section_scores = [0.0 for k in section_files]\n",
    "        for file_idx, file_path in enumerate(section_files):\n",
    "            section_scores[file_idx] = calc_anomaly_score(\n",
    "                model, file_path=file_path, section_index=section_index\n",
    "            )\n",
    "\n",
    "        section_scores = numpy.array(section_scores)\n",
    "        dataset_scores = numpy.append(dataset_scores, section_scores)\n",
    "\n",
    "    dataset_scores = numpy.array(dataset_scores)\n",
    "\n",
    "    # fit gamma distribution for anomaly scores\n",
    "    gamma_params = scipy.stats.gamma.fit(dataset_scores)\n",
    "    gamma_params = list(gamma_params)\n",
    "\n",
    "    # save the parameters of the distribution\n",
    "    score_file_path = \"{model}/score_distr_{machine_type}.pkl\".format(\n",
    "        model=CONFIG[\"model_directory\"], machine_type=os.path.split(target_dir)[1]\n",
    "    )\n",
    "    joblib.dump(gamma_params, score_file_path)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, machine_type):\n",
    "    \"\"\"\n",
    "    Save PyTorch model.\n",
    "    \"\"\"\n",
    "\n",
    "    model_file_path = \"{model}/model_{machine_type}.hdf5\".format(\n",
    "        model=model_dir, machine_type=machine_type\n",
    "    )\n",
    "    # if os.path.exists(model_file_path):\n",
    "    #     print(\"Model already exists!\")\n",
    "    #     continue\n",
    "    torch.save(model.state_dict(), model_file_path)\n",
    "    print(\"save_model -> %s\" % (model_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc6442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, data_loader, optimizer, scheduler=None):\n",
    "    \"\"\"\n",
    "    Perform training\n",
    "    \"\"\"\n",
    "    model.train()  # training mode\n",
    "    train_loss = 0.0\n",
    "    for data, label in data_loader:\n",
    "        data = data.to(DEVICE).float()\n",
    "        label = label.to(DEVICE).long()\n",
    "        optimizer.zero_grad()  # reset gradient\n",
    "        loss = model.get_loss(data, label)\n",
    "        loss.backward()  # backpropagation\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()  # update paramerters\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()  # update learning rate\n",
    "    #_ = model.get_loss(data, label, imshow=True)\n",
    "    # model.eval()\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for data, label in data_loader:\n",
    "    #         data = data.to(DEVICE).float()\n",
    "    #         label = label.to(DEVICE).long()\n",
    "    #         outputs = model.get_loss(data)\n",
    "    #         _, predicted = torch.max(outputs, 1)\n",
    "    #         correct += (predicted == label).sum().item()\n",
    "    #         total += label.size(0)\n",
    "\n",
    "    print(\"loss: {:.6f} - \".format(train_loss / len(data_loader)))\n",
    "    # print(\n",
    "    #     \"accuracy: {:.6f}% ({}/{})\".format(\n",
    "    #         100 * float(correct / total), correct, total\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "\n",
    "def validation(model, data_loader):\n",
    "    \"\"\"\n",
    "    Perform validation\n",
    "    \"\"\"\n",
    "    model.eval()  # validation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(DEVICE).float()\n",
    "            label = label.to(DEVICE).long()\n",
    "            loss = model.get_loss(data, label)\n",
    "            loss = loss.mean()\n",
    "            val_loss += loss.item()\n",
    "        #_ = model.get_loss(data, label, imshow=True)\n",
    "            #outputs = model(data)\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            # correct += (predicted == label).sum().item()\n",
    "            # total += label.size(0)\n",
    "\n",
    "    print(\"loss: {:.6f} - \".format(val_loss / len(data_loader)))\n",
    "    # print(\n",
    "    #     \"accuracy: {:.6f}% ({}/{})\".format(\n",
    "    #         100 * float(correct / total), correct, total\n",
    "    #     ),\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a741c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode):\n",
    "    \"\"\"\n",
    "    Perform model training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # check mode\n",
    "    # \"development\": mode == True\n",
    "    # \"evaluation\": mode == False\n",
    "    #mode = util.command_line_chk()  # constant: True or False\n",
    "    #if mode is None:\n",
    "    #    sys.exit(-1)\n",
    "\n",
    "    # make output directory\n",
    "    os.makedirs(CONFIG[\"model_directory\"], exist_ok=True)\n",
    "\n",
    "    # load base_directory list\n",
    "    dir_list = util.select_dirs(config=CONFIG, mode=mode)\n",
    "\n",
    "    for idx, target_dir in enumerate(dir_list):\n",
    "        print(\"===============================================\")\n",
    "        print(\"[%d/%d] %s\" % (idx + 1, len(dir_list), target_dir))\n",
    "\n",
    "        section_names_file_path = \"{model}/section_names_{machine_type}.pkl\".format(\n",
    "            model=CONFIG[\"model_directory\"], machine_type=os.path.split(target_dir)[1]\n",
    "        )\n",
    "        unique_section_names = numpy.unique(\n",
    "            util.get_section_names(target_dir, dir_name=\"train\")\n",
    "        )\n",
    "        joblib.dump(unique_section_names, section_names_file_path)\n",
    "\n",
    "        print(\"\\n============== DATASET_GENERATOR ==============\")\n",
    "        # for debug ########################################\n",
    "        #unique_section_names = unique_section_names[0:1]\n",
    "        ####################################################\n",
    "        dcase_dataset = DcaseDataset(unique_section_names, target_dir, mode)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "        print(\"\\n=========== DATALOADER_GENERATOR ==============\")\n",
    "        data_loader = {\"train\": None, \"val\": None, \"eval_train\": None}\n",
    "        (\n",
    "            data_loader[\"train\"],\n",
    "            data_loader[\"val\"],\n",
    "            data_loader[\"eval_train\"],\n",
    "        ) = get_dataloader(dcase_dataset)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "        print(\"\\n================ MODEL TRAINING ===============\")\n",
    "        model = get_model(embed_dim=CONFIG['feature']['n_mels'], seq_len=CONFIG['feature']['n_frames'])\n",
    "        optimizer, _ = get_optimizer(model)\n",
    "        # optimizer, scheduler = get_optimizer(model)  # optional\n",
    "\n",
    "        # display summary of model through torchinfo\n",
    "        #summary(\n",
    "        #    model,\n",
    "        #    input_size=(\n",
    "        #        CONFIG[\"training\"][\"batch_size\"],\n",
    "        #        1,  # number of channels\n",
    "        #        CONFIG[\"feature\"][\"n_frames\"],\n",
    "        #        CONFIG[\"feature\"][\"n_mels\"],\n",
    "        #    ),\n",
    "        #)\n",
    "\n",
    "        # training loop\n",
    "        for epoch in range(1, CONFIG[\"training\"][\"epochs\"] + 1):\n",
    "            now = datetime.datetime.now()\n",
    "            now_str = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "            print(\"{} Epoch {:2d} Train: \".format(now_str, epoch), end=\"\")\n",
    "            training(\n",
    "                model=model,\n",
    "                data_loader=data_loader[\"train\"],\n",
    "                optimizer=optimizer,\n",
    "                # scheduler=scheduler  # optional\n",
    "            )\n",
    "            #now = datetime.datetime.now()\n",
    "            #now_str = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "            print(\"{} Epoch {:2d} Valid: \".format(now_str, epoch), end=\"\")\n",
    "            validation(model=model, data_loader=data_loader[\"val\"])\n",
    "\n",
    "        del dcase_dataset, data_loader\n",
    "\n",
    "        # fit gamma distribution for anomaly scores\n",
    "        # and save the parameters of the distribution\n",
    "        #fit_gamma_dist(model=model, target_dir=target_dir, mode=mode)\n",
    "\n",
    "        print(\"============== SAVE MODEL ==============\")\n",
    "        save_model(\n",
    "            model,\n",
    "            model_dir=CONFIG[\"model_directory\"],\n",
    "            machine_type=os.path.split(target_dir)[1],\n",
    "        )\n",
    "\n",
    "        print(\"============== END TRAINING ==============\")\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5478d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_directory <- development\n",
      "===============================================\n",
      "[1/7] /media/hiroki/HDD1TB/research/dcase2021_task2/datasets_add/dev_data/ToyCar\n",
      "\n",
      "============== DATASET_GENERATOR ==============\n",
      "target_dir : /media/hiroki/HDD1TB/research/dcase2021_task2/datasets_add/dev_data/ToyCar_section_00\n",
      "number of files : 1003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba369fc5409d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-def1d5cdee8a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0munique_section_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_section_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdcase_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDcaseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_section_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===============================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-67db140a86ed>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, unique_section_names, target_dir, mode)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# extract features from audio files and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# concatenate them into Numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-67db140a86ed>\u001b[0m in \u001b[0;36mconcat_features\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# extract feature from audio file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_hop_frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/dcase2021_task2/src/model_codes/Transformer_AE/subsequence_approach/Transformer_decoder/util.py\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(file_name, config)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# generate melspectrogram using librosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# mel_spectrogram is np.ndarray [shape=(n_mels, t)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/dcase2021_task2/src/model_codes/Transformer_AE/subsequence_approach/Transformer_decoder/util.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(audio_file, mono)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmono\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_broken or not exists!! : %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Load the target number of frames, and transpose to match librosa form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                 \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0mctype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
